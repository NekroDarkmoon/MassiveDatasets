[0m[[0m[31merror[0m] [0m[0morg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (NekLenovo executor driver): java.lang.ArrayIndexOutOfBoundsException: 1[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.mllib.util.MLUtils$.$anonfun$parseLibSVMRecord$2(MLUtils.scala:135)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableLike.map(TraversableLike.scala:237)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableLike.map$(TraversableLike.scala:230)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.mllib.util.MLUtils$.parseLibSVMRecord(MLUtils.scala:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.mllib.util.MLUtils$.$anonfun$parseLibSVMFile$4(MLUtils.scala:126)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator.foreach(Iterator.scala:941)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator.foreach$(Iterator.scala:941)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:188)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:181)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$reduce$2(RDD.scala:1106)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:748)[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mDriver stacktrace:[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.foreach(Option.scala:274)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.mllib.util.MLUtils$.computeNumFeatures(MLUtils.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.source.libsvm.LibSVMFileFormat.$anonfun$inferSchema$1(LibSVMRelation.scala:105)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:138)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.ml.source.libsvm.LibSVMFileFormat.inferSchema(LibSVMRelation.scala:96)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.orElse(Option.scala:306)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:138)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)[0m
[0m[[0m[31merror[0m] [0m[0m	at stackoverflow.Topics$.delayedEndpoint$stackoverflow$Topics$1(topics.scala:30)[0m
[0m[[0m[31merror[0m] [0m[0m	at stackoverflow.Topics$delayedInit$body.apply(topics.scala:13)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Function0.apply$mcV$sp(Function0.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Function0.apply$mcV$sp$(Function0.scala:39)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.App.$anonfun$main$1$adapted(App.scala:80)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.immutable.List.foreach(List.scala:392)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.App.main(App.scala:80)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.App.main$(App.scala:78)[0m
[0m[[0m[31merror[0m] [0m[0m	at stackoverflow.Topics$.main(topics.scala:13)[0m
[0m[[0m[31merror[0m] [0m[0m	at stackoverflow.Topics.main(topics.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.reflect.Method.invoke(Method.java:498)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:748)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.lang.ArrayIndexOutOfBoundsException: 1[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.mllib.util.MLUtils$.$anonfun$parseLibSVMRecord$2(MLUtils.scala:135)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableLike.map(TraversableLike.scala:237)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableLike.map$(TraversableLike.scala:230)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.mllib.util.MLUtils$.parseLibSVMRecord(MLUtils.scala:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.mllib.util.MLUtils$.$anonfun$parseLibSVMFile$4(MLUtils.scala:126)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator.foreach(Iterator.scala:941)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator.foreach$(Iterator.scala:941)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:188)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:181)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$reduce$2(RDD.scala:1106)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:748)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (NekLenovo executor driver): java.lang.ArrayIndexOutOfBoundsException: 1[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.mllib.util.MLUtils$.$anonfun$parseLibSVMRecord$2(MLUtils.scala:135)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableLike.map(TraversableLike.scala:237)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableLike.map$(TraversableLike.scala:230)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.mllib.util.MLUtils$.parseLibSVMRecord(MLUtils.scala:132)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.mllib.util.MLUtils$.$anonfun$parseLibSVMFile$4(MLUtils.scala:126)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator.foreach(Iterator.scala:941)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.Iterator.foreach$(Iterator.scala:941)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:188)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:181)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.$anonfun$reduce$2(RDD.scala:1106)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.Task.run(Task.scala:131)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:748)[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mDriver stacktrace:[0m
